SRR = ["SRR10379721", "SRR10379722", "SRR10379723", "SRR10379724", "SRR10379725", "SRR10379726"]

raw_directory = "data_raw"
trimmed_directory = "data_trimmed"
reference_directory = "reference"
mapping_directory = "data_mapped"
count_directory = "data_counts"

rule all:
    input:
        f"{count_directory}/counts.txt"

# Basic steps of an RNA-seq analysis

# Step 1: Download and compress FASTQ files
rule download_and_compress:
    output: f"{raw_directory}/{{srr}}.fastq.gz"
    params: threads = 4
    wildcard_constraints: srr="SRR[0-9]+"
    container: "docker://paulinecoute/base:latest"
    shell:
        r"""
        mkdir -p {raw_directory}
        cd {raw_directory}
        fasterq-dump --threads {params.threads} --progress {wildcards.srr}
        pigz -p {params.threads} {wildcards.srr}.fastq
        """

# Step 2: Trim low-quality reads
rule trim_reads:
    input:
        fastq = f"{raw_directory}/{{srr}}.fastq.gz"
    output:
        f"{trimmed_directory}/{{srr}}_trimmed.fq.gz"
    params:
        quality = 20, length = 25
    wildcard_constraints: srr="SRR[0-9]+"
    container: "docker://paulinecoute/trim:latest"
    shell:
        r"""
        mkdir -p {trimmed_directory}
        cd {trimmed_directory}
        trim_galore -q {params.quality} --phred33 --length {params.length} ../{input.fastq}
        """

# Step 3: Download reference genome and annotations
rule download_reference:
    output:
        fasta = f"{reference_directory}/reference.fasta",
        gff = f"{reference_directory}/reference.gff"
    container: "docker://paulinecoute/base:latest"
    shell:
        r"""
        mkdir -p {reference_directory}
        cd {reference_directory}
        wget -q -O reference.fasta "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nuccore&id=CP000253.1&rettype=fasta&retmode=text"
        wget -q -O reference.gff "https://www.ncbi.nlm.nih.gov/sviewer/viewer.cgi?db=nuccore&id=CP000253.1&report=gff3&format=text"
        """

# Step 4: Build Bowtie index
rule build_index:
    input:
        fasta = f"{reference_directory}/reference.fasta"
    output:
        f"{reference_directory}/reference.1.ebwt"
    container: "docker://paulinecoute/bowtie:latest"
    shell:
        "cd {reference_directory} && bowtie-build reference.fasta reference"

# Step 5: Map trimmed reads to the reference genome
rule mapping:
    input:
        fastq = f"{trimmed_directory}/{{srr}}_trimmed.fq.gz",
        index = f"{reference_directory}/reference.1.ebwt"
    output:
        bam = f"{mapping_directory}/{{srr}}.bam",
        bai = f"{mapping_directory}/{{srr}}.bam.bai"
    params:
        threads = 4,
        index_name = f"{reference_directory}/reference"
    wildcard_constraints: srr="SRR[0-9]+"
    container: "docker://paulinecoute/bowtie:latest"
    shell:
        r"""
        mkdir -p {mapping_directory}
        bowtie -p {params.threads} -S {params.index_name} <(gunzip -c {input.fastq}) | \
            samtools sort -@ {params.threads} -o {output.bam}
        samtools index {output.bam}
        """

# Step 6: Count reads per gene using featureCounts
rule count_reads:
    input:
        gff = f"{reference_directory}/reference.gff",
        bam = expand(f"{mapping_directory}/{{srr}}.bam", srr=SRR)
    output:
        f"{count_directory}/counts.txt"
    params: threads = 4
    container: "docker://paulinecoute/featurecounts:latest"
    shell:
        r"""
        mkdir -p {count_directory}
        featureCounts -t gene -g ID -s 1 -F GFF -T {params.threads} \
            -a {input.gff} -o {output} {mapping_directory}/*.bam
        """
